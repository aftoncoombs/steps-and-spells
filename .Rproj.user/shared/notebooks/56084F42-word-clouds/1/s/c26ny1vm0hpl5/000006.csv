"0","sending_tokens <-"
"0","  paste(sending$Message, collapse = "" "") %>%"
"0","  tokenizers::tokenize_words(stopwords = tm::stopwords())"
"0",""
"0","sending_corp <- "
"0","  Corpus(VectorSource(sending$Message)) %>%"
"0","  tm_map(removeWords, stopwords(""english"")) %>%"
"0","  TermDocumentMatrix() %>%"
"0","  as.matrix()"
"2","transformation drops documents"
"0","sending_counts <-"
"0","  data.frame(word = rownames(sending_corp),"
"0","             freq = rowSums(sending_corp)) %>%"
"0","  dplyr::arrange(desc(freq))"
"0",""
"0","nein_tokens <- "
"0","  paste(nein$Speech, collapse = "" "") %>%"
"0","  tokenizers::tokenize_words(stopwords = tm::stopwords())"
"0",""
"0","nein_corp <- "
"0","  Corpus(VectorSource(nein$Speech)) %>%"
"0","  tm_map(removeWords, stopwords(""english"")) %>%"
"0","  TermDocumentMatrix() %>%"
"0","  as.matrix()"
"2","transformation drops documents"
"0","nein_counts <-"
"0","  data.frame(word = rownames(nein_corp),"
"0","             freq = rowSums(nein_corp)) %>%"
"0","  dplyr::arrange(desc(freq))"
"0",""
"0","kiri_corp <- "
"0","  Corpus(VectorSource(kiri_data$Words)) %>%"
"0","  tm_map(removeWords, stopwords(""english"")) %>%"
"0","  TermDocumentMatrix() %>%"
"0","  as.matrix()"
"2","transformation drops documents"
"0","kiri_counts <-"
"0","  data.frame(word = rownames(kiri_corp),"
"0","             freq = rowSums(kiri_corp)) %>%"
"0","  dplyr::arrange(desc(freq))"
"0","  "
